/* Copyright (c) 2018-2020, Stanford University
 *
 * Permission to use, copy, modify, and/or distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "Receiver.h"

#include <Cycles.h>

#include "Perf.h"
#include "Tub.h"
#include "Util.h"

namespace Homa {
namespace Core {

/**
 * Receiver constructor.
 *
 * @param driver
 *      The driver used to send and receive packets.
 * @param callbacks
 *      User-defined transport callbacks.
 * @param policyManager
 *      Provides information about the grant and network priority policies.
 * @param messageTimeoutCycles
 *      Number of cycles of inactivity to wait before this Receiver declares an
 *      Receiver::Message receive failure.
 * @param resendIntervalCycles
 *      Number of cycles of inactivity to wait between requesting retransmission
 *      of un-received parts of a message.
 */
Receiver::Receiver(Driver* driver, Transport::Callbacks* callbacks,
                   Policy::Manager* policyManager,
                   uint64_t messageTimeoutCycles, uint64_t resendIntervalCycles)
    : callbacks(callbacks)
    , driver(driver)
    , policyManager(policyManager)
    , messageBuckets(messageTimeoutCycles, resendIntervalCycles)
    , schedulerMutex()
    , scheduledPeers()
    , dontNeedGrants()
    , messageAllocator()
{}

/**
 * Receiver destructor.
 */
Receiver::~Receiver()
{
    schedulerMutex.lock();
    scheduledPeers.clear();
    peerTable.clear();
    for (auto it = messageBuckets.buckets.begin();
         it != messageBuckets.buckets.end(); ++it) {
        MessageBucket* bucket = *it;
        bucket->mutex.lock();
        auto iit = bucket->messages.begin();
        while (iit != bucket->messages.end()) {
            Message* message = &(*iit);
            bucket->messageTimeouts.cancelTimeout(&message->messageTimeout);
            bucket->resendTimeouts.cancelTimeout(&message->resendTimeout);
            iit = bucket->messages.remove(iit);
            {
                SpinLock::Lock lock_allocator(messageAllocator.mutex);
                messageAllocator.pool.destroy(message);
            }
        }
    }
}

/**
 * Process an incoming DATA packet.
 *
 * @param packet
 *      The incoming packet to be processed.
 * @param sourceIp
 *      Source IP address of the packet.
 */
void
Receiver::handleDataPacket(Driver::Packet* packet, IpAddress sourceIp)
{
    Protocol::Packet::DataHeader* header =
        static_cast<Protocol::Packet::DataHeader*>(packet->payload);
    uint16_t dataHeaderLength = sizeof(Protocol::Packet::DataHeader);
    Protocol::MessageId id = header->common.messageId;

    MessageBucket* bucket = messageBuckets.getBucket(id);
    Tub<SpinLock::Lock> lock_bucket;
    lock_bucket.construct(bucket->mutex);
    Message* message = bucket->findMessage(id, *lock_bucket);
    if (message == nullptr) {
        // New message
        int messageLength = header->totalLength;
        int numUnscheduledPackets = header->unscheduledIndexLimit;
        {
            SpinLock::Lock lock_allocator(messageAllocator.mutex);
            SocketAddress srcAddress = {
                .ip = sourceIp, .port = be16toh(header->common.prefix.sport)};
            message = messageAllocator.pool.construct(
                this, driver, dataHeaderLength, messageLength, id, srcAddress,
                numUnscheduledPackets);
        }

        bucket->messages.push_back(&message->bucketNode);
        policyManager->signalNewMessage(
            message->source.ip, header->policyVersion, header->totalLength);

        if (message->scheduled) {
            // Message needs to be scheduled.
            SpinLock::Lock lock_scheduler(schedulerMutex);
            schedule(message, lock_scheduler);
        }
    }

    // Things that must be true (sanity check)
    assert(id == message->id);
    assert(message->driver == driver);
    assert(message->source.ip == sourceIp);
    assert(message->source.port == be16toh(header->common.prefix.sport));
    assert(message->messageLength == Util::downCast<int>(header->totalLength));

    // Add the packet
    bool packetAdded = message->setPacket(header->index, packet);
    if (packetAdded) {
        // Update schedule for scheduled messages.
        if (message->scheduled) {
            SpinLock::Lock lock_scheduler(schedulerMutex);
            ScheduledMessageInfo* info = &message->scheduledMessageInfo;
            // Update the schedule if the message is still being scheduled
            // (i.e. still linked to a scheduled peer).
            if (info->peer != nullptr) {
                int packetDataBytes =
                    packet->length - message->TRANSPORT_HEADER_LENGTH;
                assert(info->bytesRemaining >= packetDataBytes);
                info->bytesRemaining -= packetDataBytes;
                updateSchedule(message, lock_scheduler);
            }

            // Non-duplicate DATA packets from scheduled messages can change
            // the state of scheduledPeers; time to run trySendGrants() again
            signalNeedGrants(lock_scheduler);
        }

        // Receiving a new packet means the message is still active so it
        // shouldn't time out until a while later.
        bucket->messageTimeouts.setTimeout(&message->messageTimeout);
        if (message->numPackets < message->numExpectedPackets) {
            // Still waiting for more packets to arrive but the arrival of a
            // new packet means we should wait a while longer before requesting
            // RESENDs of the missing packets.
            bucket->resendTimeouts.setTimeout(&message->resendTimeout);
        } else {
            // All message packets have been received.
            message->setState(Message::State::COMPLETED);
            bucket->resendTimeouts.cancelTimeout(&message->resendTimeout);
            lock_bucket.destroy();

            uint16_t dport = be16toh(header->common.prefix.dport);
            bool success =
                callbacks->deliver(dport, Homa::unique_ptr<InMessage>(message));
            if (!success) {
                ERROR("Unable to deliver the message; message dropped");
                dropMessage(message);
            }
        }
    } else {
        // must be a duplicate packet; drop packet.
        driver->releasePackets(packet, 1);
    }
}

/**
 * Process an incoming BUSY packet.
 *
 * @param packet
 *      The incoming BUSY packet to be processed.
 */
void
Receiver::handleBusyPacket(Driver::Packet* packet)
{
    Protocol::Packet::BusyHeader* header =
        static_cast<Protocol::Packet::BusyHeader*>(packet->payload);
    Protocol::MessageId id = header->common.messageId;

    MessageBucket* bucket = messageBuckets.getBucket(id);
    SpinLock::Lock lock_bucket(bucket->mutex);
    Message* message = bucket->findMessage(id, lock_bucket);
    if (message != nullptr) {
        // Sender has replied BUSY to our RESEND request; consider this message
        // still active.
        bucket->messageTimeouts.setTimeout(&message->messageTimeout);
        if (message->getState() == Message::State::IN_PROGRESS) {
            bucket->resendTimeouts.setTimeout(&message->resendTimeout);
        }
    }
    driver->releasePackets(packet, 1);
}

/**
 * Process an incoming PING packet.
 *
 * @param packet
 *      The incoming PING packet to be processed.
 * @param sourceIp
 *      Source IP address of the packet.
 */
void
Receiver::handlePingPacket(Driver::Packet* packet, IpAddress sourceIp)
{
    Protocol::Packet::PingHeader* header =
        static_cast<Protocol::Packet::PingHeader*>(packet->payload);
    Protocol::MessageId id = header->common.messageId;

    MessageBucket* bucket = messageBuckets.getBucket(id);
    SpinLock::Lock lock_bucket(bucket->mutex);
    Message* message = bucket->findMessage(id, lock_bucket);
    if (message != nullptr) {
        // Sender is checking on this message; consider it still active.
        bucket->messageTimeouts.setTimeout(&message->messageTimeout);

        // We are here either because a GRANT  got lost, or we haven't issued a
        // GRANT in along time.  Send out the latest GRANT if one exists or just
        // an "empty" GRANT to let the Sender know we are aware of the message.

        // Default to an empty GRANT.  Sending an empty GRANT will not reset the
        // priority of a Message that has not yet received a GRANT.
        int bytesGranted = 0;
        int priority = 0;

        if (message->scheduled) {
            // Use the scheduled GRANT information for scheduled messages.
            // This may still contain the default values (i.e. an empty GRANT)
            // if no GRANTs have been issued yet.
            SpinLock::Lock lock_scheduler(schedulerMutex);
            ScheduledMessageInfo* info = &message->scheduledMessageInfo;
            bytesGranted = info->bytesGranted;
            priority = info->priority;
        }

        Perf::counters.tx_grant_pkts.add(1);
        ControlPacket::send<Protocol::Packet::GrantHeader>(
            driver, message->source.ip, message->id, bytesGranted, priority);
    } else {
        // We are here because we have no knowledge of the message the Sender is
        // asking about.  Reply UNKNOWN so the Sender can react accordingly.
        Perf::counters.tx_unknown_pkts.add(1);
        ControlPacket::send<Protocol::Packet::UnknownHeader>(driver, sourceIp,
                                                             id);
    }
    driver->releasePackets(packet, 1);
}

/**
 * Process any Receiver timeouts that have expired.
 *
 * This method must be called periodically to ensure timely handling of
 * expired timeouts.
 *
 * @return
 *      The rdtsc cycle time when this method should be called again.
 */
uint64_t
Receiver::checkTimeouts()
{
    // Ping Timeout
    uint64_t resendTimeout = checkResendTimeouts();

    // Message Timeout
    uint64_t messageTimeout = checkMessageTimeouts();

    return std::min(resendTimeout, messageTimeout);
}

/**
 * Destruct a Message. Will release all contained Packet objects.
 */
Receiver::Message::~Message()
{
    // Find contiguous ranges of packets and release them back to the
    // driver.
    int num = 0;
    int index = 0;
    int packetsFound = 0;
    for (int i = 0; i < MAX_MESSAGE_PACKETS && packetsFound < numPackets; ++i) {
        if (occupied.test(i)) {
            if (num == 0) {
                // First packet in new region.
                index = i;
            }
            ++num;
            ++packetsFound;
        } else {
            if (num != 0) {
                // End of region; release the last region.
                driver->releasePackets(&packets[index], num);
                num = 0;
            }
        }
    }
    if (num != 0) {
        // Release the last region (if any).
        driver->releasePackets(&packets[index], num);
    }
}

/**
 * @copydoc Homa::InMessage::acknowledge()
 */
void
Receiver::Message::acknowledge() const
{
    MessageBucket* bucket = receiver->messageBuckets.getBucket(id);
    SpinLock::Lock lock(bucket->mutex);
    Perf::counters.tx_done_pkts.add(1);
    ControlPacket::send<Protocol::Packet::DoneHeader>(driver, source.ip, id);
}

/**
 * @copydoc Homa::InMessage::dropped()
 */
bool
Receiver::Message::dropped() const
{
    return getState() == State::DROPPED;
}

/**
 * @copydoc See Homa::InMessage::fail()
 */
void
Receiver::Message::fail() const
{
    MessageBucket* bucket = receiver->messageBuckets.getBucket(id);
    SpinLock::Lock lock(bucket->mutex);
    Perf::counters.tx_error_pkts.add(1);
    ControlPacket::send<Protocol::Packet::ErrorHeader>(driver, source.ip, id);
}

/**
 * @copydoc Homa::InMessage::get()
 */
size_t
Receiver::Message::get(size_t offset, void* destination, size_t count) const
{
    // This operation should be performed with the offset relative to the
    // logical beginning of the Message.
    int _offset = Util::downCast<int>(offset);
    int _count = Util::downCast<int>(count);
    int realOffset = _offset + start;
    int packetIndex = realOffset / PACKET_DATA_LENGTH;
    int packetOffset = realOffset % PACKET_DATA_LENGTH;
    int bytesCopied = 0;

    // Offset is passed the end of the message.
    if (realOffset >= messageLength) {
        return 0;
    }

    if (realOffset + _count > messageLength) {
        _count = messageLength - realOffset;
    }

    while (bytesCopied < _count) {
        uint32_t bytesToCopy =
            std::min(_count - bytesCopied, PACKET_DATA_LENGTH - packetOffset);
        const Driver::Packet* packet = getPacket(packetIndex);
        if (packet != nullptr) {
            char* source = static_cast<char*>(packet->payload);
            source += packetOffset + TRANSPORT_HEADER_LENGTH;
            std::memcpy(static_cast<char*>(destination) + bytesCopied, source,
                        bytesToCopy);
        } else {
            ERROR("Message is missing data starting at packet index %u",
                  packetIndex);
            break;
        }
        bytesCopied += bytesToCopy;
        packetIndex++;
        packetOffset = 0;
    }
    return bytesCopied;
}

/**
 * @copydoc Homa::InMessage::getSourceAddress()
 */
SocketAddress
Receiver::Message::getSourceAddress() const
{
    return source;
}

/**
 * @copydoc Homa::InMessage::length()
 */
size_t
Receiver::Message::length() const
{
    return Util::downCast<size_t>(messageLength - start);
}

/**
 * @copydoc Homa::InMessage::strip()
 */
void
Receiver::Message::strip(size_t count)
{
    start = std::min(start + Util::downCast<int>(count), messageLength);
}

/**
 * @copydoc Homa::InMessage::release()
 */
void
Receiver::Message::release()
{
    receiver->dropMessage(this);
}

/**
 * Return the Packet with the given index.
 *
 * @param index
 *      A Packet's index in the array of packets that form the message.
 *      "packet index = "packet message offset" / PACKET_DATA_LENGTH
 * @return
 *      Pointer to a Packet at the given index if it exists; nullptr otherwise.
 */
const Driver::Packet*
Receiver::Message::getPacket(size_t index) const
{
    if (occupied.test(index)) {
        return &packets[index];
    }
    return nullptr;
}

/**
 * Store the given packet as the Packet of the given index if one does not
 * already exist.
 *
 * Responsibly for releasing the given Packet is passed to this context if the
 * Packet is stored (returns true).
 *
 * @param index
 *      The Packet's index in the array of packets that form the message.
 *      "packet index = "packet message offset" / PACKET_DATA_LENGTH
 * @param packet
 *      The packet that should be stored.
 * @return
 *      True if the packet was stored; false if a packet already exists (the new
 *      packet is not stored).
 */
bool
Receiver::Message::setPacket(size_t index, Driver::Packet* packet)
{
    if (occupied.test(index)) {
        return false;
    }
    packets[index] = *packet;
    occupied.set(index);
    numPackets++;
    return true;
}

/**
 * Clear the atomic _dontNeedGrants_ flag to indicate that trySendGrants()
 * needs to run again. This method is called when the state of active messages
 * in Receiver::scheduledPeers might have changed.
 *
 * Note: we require the caller to hold the schedulerMutex during this call
 * because it becomes much easier to reason about the interaction between
 * the atomic flag and the mutex this way (and it's essentially free).
 *
 * @param lockHeld
 *      Reminder to hold the Receiver::schedulerMutex during this call.
 */
void
Receiver::signalNeedGrants(const SpinLock::Lock& lockHeld)
{
    (void)lockHeld;
    dontNeedGrants.clear(std::memory_order_release);
}

/**
 * Inform the Receiver that an Message returned by receiveMessage() is not
 * needed and can be dropped.
 *
 * @param message
 *      Message which will be dropped.
 */
void
Receiver::dropMessage(Receiver::Message* message)
{
    Protocol::MessageId msgId = message->id;
    MessageBucket* bucket = messageBuckets.getBucket(msgId);
    SpinLock::Lock lock_bucket(bucket->mutex);
    Message* foundMessage = bucket->findMessage(msgId, lock_bucket);
    if (foundMessage != nullptr) {
        assert(message == foundMessage);
        bucket->messageTimeouts.cancelTimeout(&message->messageTimeout);
        bucket->resendTimeouts.cancelTimeout(&message->resendTimeout);
        if (message->scheduled) {
            // Unschedule the message if it is still scheduled (i.e. still
            // linked to a scheduled peer).
            SpinLock::Lock lock_scheduler(schedulerMutex);
            ScheduledMessageInfo* info = &message->scheduledMessageInfo;
            if (info->peer != nullptr) {
                unschedule(message, lock_scheduler);
            }
        }
        bucket->messages.remove(&message->bucketNode);
        {
            SpinLock::Lock lock_allocator(messageAllocator.mutex);
            messageAllocator.pool.destroy(message);
        }
    }
}

/**
 * Process any inbound messages that have timed out due to lack of activity from
 * the Sender.
 *
 * Pulled out of checkTimeouts() for ease of testing.
 *
 * @return
 *      The rdtsc cycle time when this method should be called again.
 */
uint64_t
Receiver::checkMessageTimeouts()
{
    uint64_t globalNextTimeout = UINT64_MAX;
    for (int i = 0; i < MessageBucketMap::NUM_BUCKETS; ++i) {
        MessageBucket* bucket = messageBuckets.buckets.at(i);
        uint64_t nextTimeout = 0;
        while (true) {
            SpinLock::Lock lock_bucket(bucket->mutex);

            // No remaining timeouts.
            if (bucket->messageTimeouts.list.empty()) {
                nextTimeout = PerfUtils::Cycles::rdtsc() +
                              bucket->messageTimeouts.timeoutIntervalCycles;
                break;
            }

            Message* message = &bucket->messageTimeouts.list.front();

            // No remaining expired timeouts.
            if (!message->messageTimeout.hasElapsed()) {
                nextTimeout = message->messageTimeout.expirationCycleTime;
                break;
            }

            // Found expired timeout.

            // Cancel timeouts
            bucket->messageTimeouts.cancelTimeout(&message->messageTimeout);
            bucket->resendTimeouts.cancelTimeout(&message->resendTimeout);

            if (message->getState() == Message::State::IN_PROGRESS) {
                // Message timed out before being fully received; drop the
                // message.

                // Unschedule the message
                if (message->scheduled) {
                    // Unschedule the message if it is still scheduled (i.e.
                    // still linked to a scheduled peer).
                    SpinLock::Lock lock_scheduler(schedulerMutex);
                    ScheduledMessageInfo* info = &message->scheduledMessageInfo;
                    if (info->peer != nullptr) {
                        unschedule(message, lock_scheduler);
                    }
                }

                bucket->messages.remove(&message->bucketNode);
                {
                    SpinLock::Lock lock_allocator(messageAllocator.mutex);
                    messageAllocator.pool.destroy(message);
                }
            } else {
                // Message timed out but we already made it available to the
                // Transport; let the Transport know.
                message->setState(Message::State::DROPPED);
            }
        }
        globalNextTimeout = std::min(globalNextTimeout, nextTimeout);
    }
    return globalNextTimeout;
}

/**
 * Process any inbound messages that may need to issue resends.
 *
 * Pulled out of checkTimeouts() for ease of testing.
 *
 * @return
 *      The rdtsc cycle time when this method should be called again.
 */
uint64_t
Receiver::checkResendTimeouts()
{
    uint64_t globalNextTimeout = UINT64_MAX;
    for (int i = 0; i < MessageBucketMap::NUM_BUCKETS; ++i) {
        MessageBucket* bucket = messageBuckets.buckets.at(i);
        uint64_t nextTimeout = 0;
        while (true) {
            SpinLock::Lock lock_bucket(bucket->mutex);

            // No remaining timeouts.
            if (bucket->resendTimeouts.list.empty()) {
                nextTimeout = PerfUtils::Cycles::rdtsc() +
                              bucket->resendTimeouts.timeoutIntervalCycles;
                break;
            }

            Message* message = &bucket->resendTimeouts.list.front();

            // No remaining expired timeouts.
            if (!message->resendTimeout.hasElapsed()) {
                nextTimeout = message->resendTimeout.expirationCycleTime;
                break;
            }

            // Found expired timeout.
            assert(message->getState() == Message::State::IN_PROGRESS);
            bucket->resendTimeouts.setTimeout(&message->resendTimeout);

            // This Receiver expected to have heard from the Sender within the
            // last timeout period but it didn't.  Request a resend of granted
            // packets in case DATA packets got lost.
            int index = 0;
            int num = 0;
            int grantIndexLimit = message->numUnscheduledPackets;

            if (message->scheduled) {
                SpinLock::Lock lock_scheduler(schedulerMutex);
                ScheduledMessageInfo* info = &message->scheduledMessageInfo;
                int receivedBytes = info->messageLength - info->bytesRemaining;
                if (receivedBytes >= info->bytesGranted) {
                    // Sender is blocked on this Receiver; all granted packets
                    // have already been received.  No need to check for resend.
                    continue;
                } else if (grantIndexLimit * message->PACKET_DATA_LENGTH <
                           info->bytesGranted) {
                    grantIndexLimit =
                        (info->bytesGranted + message->PACKET_DATA_LENGTH - 1) /
                        message->PACKET_DATA_LENGTH;
                }
            }

            for (int i = 0; i < grantIndexLimit; ++i) {
                if (message->getPacket(i) == nullptr) {
                    // Unreceived packet
                    if (num == 0) {
                        // First unreceived packet
                        index = i;
                    }
                    ++num;
                } else {
                    // Received packet
                    if (num != 0) {
                        // Send out the range of packets found so far.
                        //
                        // The RESEND also includes the current granted priority
                        // so that it can act as a GRANT in case a GRANT was
                        // lost.  If this message hasn't been scheduled (i.e. no
                        // grants have been sent) then the priority will hold
                        // the default value; this is ok since the Sender will
                        // ignore the priority field for resends of purely
                        // unscheduled packets (see
                        // Sender::handleResendPacket()).
                        SpinLock::Lock lock_scheduler(schedulerMutex);
                        Perf::counters.tx_resend_pkts.add(1);
                        ControlPacket::send<Protocol::Packet::ResendHeader>(
                            message->driver, message->source.ip, message->id,
                            Util::downCast<uint16_t>(index),
                            Util::downCast<uint16_t>(num),
                            message->scheduledMessageInfo.priority);
                        num = 0;
                    }
                }
            }
            if (num != 0) {
                // Send out the last range of packets found.
                SpinLock::Lock lock_scheduler(schedulerMutex);
                Perf::counters.tx_resend_pkts.add(1);
                ControlPacket::send<Protocol::Packet::ResendHeader>(
                    message->driver, message->source.ip, message->id,
                    Util::downCast<uint16_t>(index),
                    Util::downCast<uint16_t>(num),
                    message->scheduledMessageInfo.priority);
            }
        }
        globalNextTimeout = std::min(globalNextTimeout, nextTimeout);
    }
    return globalNextTimeout;
}

/**
 * Send GRANTs to incoming Message according to the Receiver's policy.
 *
 * This method must be called eagerly to allow the Receiver to make progress
 * toward receiving incoming messages.
 *
 * @return
 *      True if the method has found some messages to grant; false, otherwise.
 */
bool
Receiver::trySendGrants()
{
    uint64_t start_tsc = PerfUtils::Cycles::rdtsc();

    // Fast path: skip if no message is waiting for grants
    bool needGrants = !dontNeedGrants.test_and_set(std::memory_order_acquire);
    if (!needGrants) {
        return false;
    }

    /* It's possible to have a benign race-condition here when another thread
     * acquires the schedulerMutex before us and sets _dontNeedGrants_ back to
     * false via signalNeedGrants. As a result, _dontNeedGrants_ will stay false
     * when the method returns although all messages have been granted.
     */
    SpinLock::Lock lock(schedulerMutex);

    /* The overall goal is to grant up to policy.degreeOvercommitment number of
     * scheduled messages simultaneously.  Each of these messages should always
     * have at least policy.minScheduledBytes number of bytes granted.  Ideally,
     * each message will be assigned a different network priority based on a
     * message's number of bytesRemaining.  The message with the fewest
     * bytesRemaining (SRPT) will be assigned the highest priority.  If the
     * number of messages to grant exceeds the number of available priorities,
     * the lowest priority is shared by multiple messages.  If the number of
     * messages to grant is fewer than the available priorities, than the
     * messages are assigned to the lowest available priority.
     */
    Policy::Scheduled policy = policyManager->getScheduledPolicy();
    assert(policy.degreeOvercommitment > policy.maxScheduledPriority);
    assert(policy.minScheduledBytes <= policy.minScheduledBytes);
    int unusedPriorities =
        std::max(0, (policy.maxScheduledPriority + 1) -
                        Util::downCast<int>(scheduledPeers.size()));

    auto it = scheduledPeers.begin();
    int slot = 0;
    bool foundWork = false;
    while (it != scheduledPeers.end() && slot < policy.degreeOvercommitment) {
        assert(!it->scheduledMessages.empty());
        Message* message = &it->scheduledMessages.front();
        ScheduledMessageInfo* info = &message->scheduledMessageInfo;
        // Access message const variables without message mutex.
        const Protocol::MessageId id = message->id;
        const IpAddress sourceIp = message->source.ip;

        // Recalculate message priority
        info->priority =
            std::max(0, policy.maxScheduledPriority - slot - unusedPriorities);

        // Send a GRANT if there are too few bytes granted and unreceived.
        int receivedBytes = info->messageLength - info->bytesRemaining;
        if (info->bytesGranted - receivedBytes < policy.minScheduledBytes) {
            // Calculate new grant limit
            int newGrantLimit = std::min(
                receivedBytes + policy.maxScheduledBytes, info->messageLength);
            assert(newGrantLimit >= info->bytesGranted);
            info->bytesGranted = newGrantLimit;
            Perf::counters.tx_grant_pkts.add(1);
            ControlPacket::send<Protocol::Packet::GrantHeader>(
                driver, sourceIp, id,
                Util::downCast<uint32_t>(info->bytesGranted), info->priority);
            foundWork = true;
        }

        // Update the iterator first since calling unschedule() may cause the
        // iterator to become invalid.
        ++it;

        if (info->messageLength <= info->bytesGranted) {
            // All packets granted, unschedule the message.
            unschedule(message, lock);
        }

        ++slot;
    }

    uint64_t elapsed_cycles = PerfUtils::Cycles::rdtsc() - start_tsc;
    if (foundWork) {
        Perf::counters.active_cycles.add(elapsed_cycles);
    } else {
        Perf::counters.idle_cycles.add(elapsed_cycles);
    }
    return foundWork;
}

/**
 * Add a Message to the schedule.
 *
 * Helper function separated mostly for ease of testing.
 *
 * @param message
 *      Message to be added.
 * @param lock
 *      Reminder to hold the Receiver::schedulerMutex during this call.
 */
void
Receiver::schedule(Receiver::Message* message, const SpinLock::Lock& lock)
{
    (void)lock;
    ScheduledMessageInfo* info = &message->scheduledMessageInfo;
    Peer* peer = &peerTable[message->source.ip];
    // Insert the Message
    peer->scheduledMessages.push_front(&info->scheduledMessageNode);
    Intrusive::deprioritize<Message>(&peer->scheduledMessages,
                                     &info->scheduledMessageNode,
                                     ScheduledMessageInfo::ComparePriority());
    info->peer = peer;
    if (!scheduledPeers.contains(&peer->scheduledPeerNode)) {
        // Must be the only message of this peer; push the peer to the
        // end of list to be moved later.
        assert(peer->scheduledMessages.size() == 1);
        scheduledPeers.push_front(&peer->scheduledPeerNode);
        Intrusive::deprioritize<Peer>(&scheduledPeers, &peer->scheduledPeerNode,
                                      Peer::ComparePriority());
    } else if (&info->peer->scheduledMessages.front() == message) {
        // Update the Peer's position in the queue since the new message is the
        // peer's first scheduled message.
        Intrusive::prioritize<Peer>(&scheduledPeers,
                                    &info->peer->scheduledPeerNode,
                                    Peer::ComparePriority());
    } else {
        // The peer's first scheduled message did not change.  Nothing to do.
    }
}

/**
 * Remove a Message from the schedule.
 *
 * Helper function separated mostly for ease of testing.
 *
 * @param message
 *      Message to be removed.
 * @param lock
 *      Reminder to hold the Receiver::schedulerMutex during this call.
 */
void
Receiver::unschedule(Receiver::Message* message, const SpinLock::Lock& lock)
{
    (void)lock;
    ScheduledMessageInfo* info = &message->scheduledMessageInfo;
    assert(info->peer != nullptr);
    Peer* peer = info->peer;
    Intrusive::List<Peer>::Iterator it =
        scheduledPeers.get(&peer->scheduledPeerNode);
    Peer::ComparePriority comp;

    // Remove message.
    assert(peer->scheduledMessages.contains(&info->scheduledMessageNode));
    peer->scheduledMessages.remove(&info->scheduledMessageNode);
    info->peer = nullptr;

    // Cleanup the schedule
    if (peer->scheduledMessages.empty()) {
        // Remove the empty peer.
        scheduledPeers.remove(it);
    } else if (std::next(it) == scheduledPeers.end() ||
               !comp(*std::next(it), *it)) {
        // Peer already in the right place (peer incremented as part of the
        // check).  Note that only "next" needs be checked (and not "prev")
        // since removing a message cannot increase the peer's priority.
    } else {
        // Peer needs to be moved.
        Intrusive::deprioritize<Peer>(&scheduledPeers, &peer->scheduledPeerNode,
                                      comp);
    }

    // scheduledPeers has been updated; time to run trySendGrants() again
    signalNeedGrants(lock);
}

/**
 * Update Message's position in the schedule.
 *
 * Called when new data has arrived for the Message.
 *
 * Helper function separated mostly for ease of testing.
 *
 * @param message
 *      Message whose position should be updated.
 * @param lock
 *      Reminder to hold the Receiver::schedulerMutex during this call.
 */
void
Receiver::updateSchedule(Receiver::Message* message, const SpinLock::Lock& lock)
{
    (void)lock;
    ScheduledMessageInfo* info = &message->scheduledMessageInfo;
    assert(info->peer != nullptr);
    assert(info->peer->scheduledMessages.contains(&info->scheduledMessageNode));

    // Update the message's position within its Peer scheduled message queue.
    Intrusive::prioritize<Message>(&info->peer->scheduledMessages,
                                   &info->scheduledMessageNode,
                                   ScheduledMessageInfo::ComparePriority());

    // Update the Peer's position in the queue if this message is now the first
    // scheduled message.
    if (&info->peer->scheduledMessages.front() == message) {
        Intrusive::prioritize<Peer>(&scheduledPeers,
                                    &info->peer->scheduledPeerNode,
                                    Peer::ComparePriority());
    }
}

}  // namespace Core
}  // namespace Homa
